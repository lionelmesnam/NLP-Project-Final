import os
import json
import re
import unicodedata

import numpy as np
import torch
import streamlit as st
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# =========================
# C·∫•u h√¨nh Streamlit
# =========================
st.set_page_config(
    page_title="VSMEC Demo - PhoBERT / XLM-R / mBERT",
    page_icon="ü§ñ",
    layout="wide",
)

st.title("ü§ñ Demo ph√¢n lo·∫°i c·∫£m x√∫c VSMEC")
st.write(
    "Nh·∫≠p c√¢u ti·∫øng Vi·ªát, 3 m√¥ h√¨nh PhoBERT, XLM-R, mBERT (ƒë√£ fine-tune) "
    "s·∫Ω c√πng d·ª± ƒëo√°n s·∫Øc th√°i c·∫£m x√∫c."
)

# =========================
# Ti·ªÅn x·ª≠ l√Ω + t√°ch t·ª´
# =========================

URL_RE = re.compile(r"(https?://\S+|www\.\S+)")
MULTISPACE_RE = re.compile(r"\s+")

def normalize_text(text: str) -> str:
    if text is None:
        return ""
    text = unicodedata.normalize("NFC", str(text))
    text = URL_RE.sub(" <url> ", text)
    text = MULTISPACE_RE.sub(" ", text).strip()
    return text

# underthesea cho PhoBERT
try:
    from underthesea import word_tokenize as uts_word_tokenize
    USE_UTS = True
    uts_error = None
except Exception as e:
    USE_UTS = False
    uts_error = str(e)

def underthesea_segment(text: str) -> str:
    text = normalize_text(text)
    if not text:
        return text
    if USE_UTS:
        try:
            seg = uts_word_tokenize(text, format="text")
            if isinstance(seg, list):
                seg = " ".join(seg)
            return MULTISPACE_RE.sub(" ", seg).strip()
        except Exception:
            pass
    # fallback: t√°ch theo kho·∫£ng tr·∫Øng
    return " ".join(text.split())


# =========================
# Load model t·ª´ th∆∞ m·ª•c local
# =========================

@st.cache_resource
def load_model_bundle(model_dir: str, device: str, use_fast: bool = True):
    """
    Load model + tokenizer + labels t·ª´ th∆∞ m·ª•c model_dir.
    Tr·∫£ v·ªÅ dict: {model, tokenizer, id2label, label_list, device}
    """
    tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=use_fast)
    model = AutoModelForSequenceClassification.from_pretrained(model_dir)
    model.to(device)
    model.eval()

    labels_path = os.path.join(model_dir, "labels.json")
    if os.path.exists(labels_path):
        with open(labels_path, "r", encoding="utf-8") as f:
            label_info = json.load(f)
        label_list = label_info.get("label_list")
        id2label = label_info.get("id2label")
        if id2label is not None:
            id2label = {int(k): v for k, v in id2label.items()}
        else:
            id2label = model.config.id2label
            label_list = [id2label[i] for i in range(model.config.num_labels)]
    else:
        id2label = model.config.id2label
        label_list = [id2label[i] for i in range(model.config.num_labels)]

    return {
        "model": model,
        "tokenizer": tokenizer,
        "id2label": id2label,
        "label_list": label_list,
        "device": device,
        "dir": model_dir,
    }


# =========================
# H√†m d·ª± ƒëo√°n 1 c√¢u
# =========================

@torch.inference_mode()
def predict_one(text: str, bundle, max_length: int = 128, use_seg: bool = False):
    """
    D·ª± ƒëo√°n 1 c√¢u v·ªõi 1 model.
    use_seg=True -> d√πng underthesea_segment, ng∆∞·ª£c l·∫°i d√πng normalize_text.
    """
    model = bundle["model"]
    tokenizer = bundle["tokenizer"]
    id2label = bundle["id2label"]
    device = bundle["device"]
    label_list = bundle["label_list"]

    if use_seg:
        processed = underthesea_segment(text)
    else:
        processed = normalize_text(text)

    if not processed:
        return {
            "input": text,
            "processed": processed,
            "pred_label": None,
            "pred_conf": 0.0,
            "probs": {lab: 0.0 for lab in label_list},
        }

    enc = tokenizer(
        processed,
        truncation=True,
        max_length=max_length,
        padding=True,
        return_tensors="pt",
    ).to(device)

    outputs = model(**enc)
    probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()[0]

    pred_id = int(np.argmax(probs))
    pred_label = id2label[pred_id]
    prob_dict = {id2label[i]: float(probs[i]) for i in range(len(probs))}

    return {
        "input": text,
        "processed": processed,
        "pred_label": pred_label,
        "pred_conf": float(probs[pred_id]),
        "probs": prob_dict,
    }


def format_topk(probs_dict, k=3):
    items = sorted(probs_dict.items(), key=lambda x: x[1], reverse=True)[:k]
    return "\n".join([f"{lbl}: {p:.3f}" for lbl, p in items])


# =========================
# Sidebar config
# =========================

st.sidebar.header("‚öôÔ∏è C·∫•u h√¨nh")

use_cuda_sidebar = st.sidebar.checkbox("D√πng GPU n·∫øu c√≥", value=True)
device_str = "cuda" if use_cuda_sidebar and torch.cuda.is_available() else "cpu"
st.sidebar.info(f"Device ƒëang d√πng: `{device_str}`")

max_len = st.sidebar.slider("Max sequence length", 32, 256, 128, step=8)

pho_dir_default   = "phobert_vsmec_best_model"
xlmr_dir_default  = "xlmr_vsmec_best_model"
mbert_dir_default = "mbert_vsmec_best_model"

st.sidebar.markdown("### Th∆∞ m·ª•c model")
pho_dir = st.sidebar.text_input("PhoBERT", value=pho_dir_default)
xlmr_dir = st.sidebar.text_input("XLM-R", value=xlmr_dir_default)
mbert_dir = st.sidebar.text_input("mBERT", value=mbert_dir_default)

st.sidebar.markdown("### Ch·ªçn m√¥ h√¨nh s·ª≠ d·ª•ng")
use_phobert = st.sidebar.checkbox("D√πng PhoBERT", value=True)
use_xlmr    = st.sidebar.checkbox("D√πng XLM-R", value=False)
use_mbert   = st.sidebar.checkbox("D√πng mBERT", value=False)

if USE_UTS:
    st.sidebar.success("underthesea: ƒëang d√πng ƒë·ªÉ t√°ch t·ª´ cho PhoBERT.")
else:
    st.sidebar.warning("underthesea KH√îNG d√πng ƒë∆∞·ª£c, PhoBERT s·∫Ω t√°ch t·ª´ ƒë∆°n gi·∫£n.")
    if uts_error:
        with st.sidebar.expander("Chi ti·∫øt l·ªói underthesea"):
            st.code(uts_error)

# =========================
# Ki·ªÉm tra th∆∞ m·ª•c + load model ƒë∆∞·ª£c ch·ªçn
# =========================

bundles = {}

try:
    if use_phobert:
        if not os.path.isdir(pho_dir):
            st.error(f"Th∆∞ m·ª•c PhoBERT kh√¥ng t·ªìn t·∫°i: `{pho_dir}`")
            st.stop()
        bundles["PhoBERT"] = load_model_bundle(pho_dir, device=device_str, use_fast=False)

    if use_xlmr:
        if not os.path.isdir(xlmr_dir):
            st.error(f"Th∆∞ m·ª•c XLM-R kh√¥ng t·ªìn t·∫°i: `{xlmr_dir}`")
            st.stop()
        bundles["XLM-R"] = load_model_bundle(xlmr_dir, device=device_str, use_fast=True)

    if use_mbert:
        if not os.path.isdir(mbert_dir):
            st.error(f"Th∆∞ m·ª•c mBERT kh√¥ng t·ªìn t·∫°i: `{mbert_dir}`")
            st.stop()
        bundles["mBERT"] = load_model_bundle(mbert_dir, device=device_str, use_fast=True)
except Exception as e:
    st.error(f"Kh√¥ng load ƒë∆∞·ª£c model.\nChi ti·∫øt l·ªói:\n{e}")
    st.stop()

if not bundles:
    st.warning("B·∫°n ch∆∞a ch·ªçn m√¥ h√¨nh n√†o ·ªü sidebar. H√£y b·∫≠t √≠t nh·∫•t 1 checkbox.")
    st.stop()
else:
    st.sidebar.success(f"ƒê√£ load {len(bundles)} m√¥ h√¨nh: {', '.join(bundles.keys())}")


# =========================
# Giao di·ªán nh·∫≠p c√¢u
# =========================

st.markdown("### Nh·∫≠p c√¢u c·∫ßn d·ª± ƒëo√°n")

sample_text = "ph·∫•n ch·∫•n l√™n n√†o bro, ·ªü ƒë√¢y c√≥ anh em, kh√¥ng ph·∫£i lo"
input_text = st.text_area(
    "M·ªói d√≤ng l√† m·ªôt c√¢u (nhi·ªÅu d√≤ng = d·ª± ƒëo√°n nhi·ªÅu c√¢u c√πng l√∫c):",
    height=150,
    value=sample_text,
)

if st.button("üöÄ D·ª± ƒëo√°n"):
    if not input_text.strip():
        st.warning("Vui l√≤ng nh·∫≠p √≠t nh·∫•t m·ªôt c√¢u.")
    else:
        sentences = [line.strip() for line in input_text.split("\n") if line.strip()]

        st.markdown("## K·∫øt qu·∫£")

        for idx, sent in enumerate(sentences, start=1):
            st.markdown("---")
            st.markdown(f"### C√¢u {idx}")
            st.markdown(f"**C√¢u g·ªëc:** {sent}")

            with st.spinner("ƒêang d·ª± ƒëo√°n..."):
                # chu·∫©n b·ªã columns t∆∞∆°ng ·ª©ng s·ªë model
                model_names = list(bundles.keys())
                cols = st.columns(len(model_names))

                for col_idx, name in enumerate(model_names):
                    bundle = bundles[name]
                    use_seg = (name == "PhoBERT")  # ch·ªâ PhoBERT d√πng underthesea
                    res = predict_one(
                        sent,
                        bundle,
                        max_length=max_len,
                        use_seg=use_seg,
                    )

                    with cols[col_idx]:
                        st.subheader(name)
                        st.markdown(f"**Nh√£n:** `{res['pred_label']}`")
                        if name == "PhoBERT":
                            st.caption("Sau khi t√°ch t·ª´:")
                        else:
                            st.caption("Sau khi chu·∫©n ho√°:")
                        st.code(res["processed"])
                        st.markdown("**Top-3 x√°c su·∫•t:**")
                        st.code(format_topk(res["probs"], k=3))

st.markdown("---")
st.caption("Demo ph√¢n lo·∫°i c·∫£m x√∫c VSMEC ‚Ä¢ PhoBERT / XLM-R / mBERT (fine-tuned)")
